{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb66e266-b93b-4242-978f-9c1b6fb9a726",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **This Notebook is to send this data to big query warehouse made from the Models Cars Cleaned Data **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e53b5da-8850-414c-be6b-346e963f0442",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **Install G cloud libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d37cf3-4480-49a9-93c6-489b695e6462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n  Obtaining dependency information for google-cloud-bigquery from https://files.pythonhosted.org/packages/2a/91/e1c80ae2924efc047ca156662d6b0458d9a9ce99204ae7e719ff9a66123d/google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata\n  Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata (8.7 kB)\nCollecting pandas-gbq\n  Obtaining dependency information for pandas-gbq from https://files.pythonhosted.org/packages/a9/eb/f35416f452de18575b216aaf4b4710beb7201dce4e07265c5134e1d1ecfd/pandas_gbq-0.23.2-py2.py3-none-any.whl.metadata\n  Downloading pandas_gbq-0.23.2-py2.py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: google-api-core[grpc]<3.0.0dev,>=2.11.1 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-bigquery) (2.18.0)\nRequirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-bigquery) (2.21.0)\nRequirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-bigquery) (2.4.1)\nRequirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-bigquery) (2.7.1)\nRequirement already satisfied: packaging>=20.0.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-bigquery) (23.2)\nRequirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-bigquery) (2.8.2)\nRequirement already satisfied: requests<3.0.0dev,>=2.21.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-bigquery) (2.31.0)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.11/site-packages (from pandas-gbq) (68.0.0)\nCollecting db-dtypes<2.0.0,>=1.0.4 (from pandas-gbq)\n  Obtaining dependency information for db-dtypes<2.0.0,>=1.0.4 from https://files.pythonhosted.org/packages/89/d0/f746b7f88e3f6e6c6753fb7a078d7d2db2b8f969c7913d51b3a54f0abe53/db_dtypes-1.3.0-py2.py3-none-any.whl.metadata\n  Downloading db_dtypes-1.3.0-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.18.1 in /databricks/python3/lib/python3.11/site-packages (from pandas-gbq) (1.23.5)\nRequirement already satisfied: pandas>=1.1.4 in /databricks/python3/lib/python3.11/site-packages (from pandas-gbq) (1.5.3)\nRequirement already satisfied: pyarrow>=3.0.0 in /databricks/python3/lib/python3.11/site-packages (from pandas-gbq) (14.0.1)\nCollecting pydata-google-auth>=1.5.0 (from pandas-gbq)\n  Obtaining dependency information for pydata-google-auth>=1.5.0 from https://files.pythonhosted.org/packages/28/6b/3320c9ddbfc572108917e8432a07e8bd1e40054d94b5ad40c755afdc1160/pydata_google_auth-1.8.2-py2.py3-none-any.whl.metadata\n  Downloading pydata_google_auth-1.8.2-py2.py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: google-auth-oauthlib>=0.7.0 in /databricks/python3/lib/python3.11/site-packages (from pandas-gbq) (1.0.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.63.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /databricks/python3/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (4.24.1)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.24.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /databricks/python3/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.60.0)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /databricks/python3/lib/python3.11/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.60.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.4.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\nRequirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (1.26.16)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /databricks/python3/lib/python3.11/site-packages (from google-auth-oauthlib>=0.7.0->pandas-gbq) (1.3.1)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from google-resumable-media<3.0dev,>=2.0.0->google-cloud-bigquery) (1.5.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas>=1.1.4->pandas-gbq) (2022.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2023.7.22)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.7.0->pandas-gbq) (3.2.0)\nDownloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl (239 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/239.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m225.3/239.1 kB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.1/239.1 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pandas_gbq-0.23.2-py2.py3-none-any.whl (28 kB)\nDownloading db_dtypes-1.3.0-py2.py3-none-any.whl (17 kB)\nDownloading pydata_google_auth-1.8.2-py2.py3-none-any.whl (15 kB)\nInstalling collected packages: db-dtypes, pydata-google-auth, google-cloud-bigquery, pandas-gbq\nSuccessfully installed db-dtypes-1.3.0 google-cloud-bigquery-3.26.0 pandas-gbq-0.23.2 pydata-google-auth-1.8.2\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-bigquery pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d8fe6c-e423-441e-b9b5-67a8dfa56e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d1545d-05d1-4830-94fa-b39bfd455747",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **Spark Write to Big Query**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768f56d0-930c-4bf2-a470-34a9caafcd24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from cleaned_customers.csv:\n+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+\n|customerNumber|        customerName|contactLastName|contactFirstName|       phone|        addressLine1|addressLine2|     city|   state|postalCode|  country|salesRepEmployeeNumber|creditLimit|\n+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+\n|           103|   Atelier graphique|        Schmitt|         Carine |  40.32.2555|      54, rue Royale|        NULL|   Nantes|    None|     44000|   France|                1370.0|    21000.0|\n|           112|  Signal Gift Stores|           King|            Jean|  7025551838|     8489 Strong St.|        NULL|Las Vegas|      NV|     83030|      USA|                1166.0|    71800.0|\n|           114|Australian Collec...|       Ferguson|           Peter|03 9520 4555|   636 St Kilda Road|     Level 3|Melbourne|Victoria|      3004|Australia|                1611.0|   117300.0|\n|           119|   La Rochelle Gifts|        Labrune|         Janine |  40.67.8555|67, rue des Cinqu...|        NULL|   Nantes|    None|     44000|   France|                1370.0|   118200.0|\n|           121|  Baane Mini Imports|     Bergulfsen|          Jonas |  07-98 9555|Erling Skakkes ga...|        NULL|  Stavern|    None|      4110|   Norway|                1504.0|    81700.0|\n+--------------+--------------------+---------------+----------------+------------+--------------------+------------+---------+--------+----------+---------+----------------------+-----------+\nonly showing top 5 rows\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.customers\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 11848.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_customers.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.customers\nLoaded data from cleaned_employees.csv:\n+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\n|employeeNumber| lastName|firstName|extension|               email|officeCode|reportsTo|            jobTitle|\n+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\n|          1002|   Murphy|    Diane|    x5800|dmurphy@classicmo...|         1|     NULL|           President|\n|          1056|Patterson|     Mary|    x4611|mpatterso@classic...|         1|   1002.0|            VP Sales|\n|          1076| Firrelli|     Jeff|    x9273|jfirrelli@classic...|         1|   1002.0|        VP Marketing|\n|          1088|Patterson|  William|    x4871|wpatterson@classi...|         6|   1056.0|Sales Manager (APAC)|\n|          1102|   Bondur|   Gerard|    x5408|gbondur@classicmo...|         4|   1056.0| Sale Manager (EMEA)|\n+--------------+---------+---------+---------+--------------------+----------+---------+--------------------+\nonly showing top 5 rows\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.employees\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 10565.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_employees.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.employees\nLoaded data from cleaned_offices.csv:\n+----------+-------------+---------------+--------------------+------------+----------+-------+----------+---------+\n|officeCode|         city|          phone|        addressLine1|addressLine2|     state|country|postalCode|territory|\n+----------+-------------+---------------+--------------------+------------+----------+-------+----------+---------+\n|         1|San Francisco|+1 650 219 4782|   100 Market Street|   Suite 300|        CA|    USA|     94080|       NA|\n|         2|       Boston|+1 215 837 0825|    1550 Court Place|   Suite 102|        MA|    USA|     02107|       NA|\n|         3|          NYC|+1 212 555 3000|523 East 53rd Street|     apt. 5A|        NY|    USA|     10022|       NA|\n|         4|        Paris|+33 14 723 4404|43 Rue Jouffroy D...|        NULL|      None| France|     75017|     EMEA|\n|         5|        Tokyo|+81 33 224 5000|         4-1 Kioicho|        NULL|Chiyoda-Ku|  Japan|  102-8578|    Japan|\n+----------+-------------+---------------+--------------------+------------+----------+-------+----------+---------+\nonly showing top 5 rows\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.offices\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 6355.01it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_offices.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.offices\nLoaded data from cleaned_orderdetails.csv:\n+-----------+-----------+---------------+---------+---------------+\n|orderNumber|productCode|quantityOrdered|priceEach|orderLineNumber|\n+-----------+-----------+---------------+---------+---------------+\n|      10100|   S18_1749|             30|    136.0|              3|\n|      10100|   S18_2248|             50|    55.09|              2|\n|      10100|   S18_4409|             22|    75.46|              4|\n|      10100|   S24_3969|             49|    35.29|              1|\n|      10101|   S18_2325|             25|   108.06|              4|\n+-----------+-----------+---------------+---------+---------------+\nonly showing top 5 rows\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.orderdetails\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 9892.23it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_orderdetails.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.orderdetails\nLoaded data from cleaned_orders.csv:\n+-----------+----------+------------+-----------+-------+--------------------+--------------+\n|orderNumber| orderDate|requiredDate|shippedDate| status|            comments|customerNumber|\n+-----------+----------+------------+-----------+-------+--------------------+--------------+\n|      10100|2003-01-06|  2003-01-13| 2003-01-10|Shipped|                None|           363|\n|      10101|2003-01-09|  2003-01-18| 2003-01-11|Shipped|Check on availabi...|           128|\n|      10102|2003-01-10|  2003-01-18| 2003-01-14|Shipped|                None|           181|\n|      10103|2003-01-29|  2003-02-07| 2003-02-02|Shipped|                None|           121|\n|      10104|2003-01-31|  2003-02-09| 2003-02-01|Shipped|                None|           141|\n+-----------+----------+------------+-----------+-------+--------------------+--------------+\nonly showing top 5 rows\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.orders\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 9576.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_orders.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.orders\nLoaded data from cleaned_payments.csv:\n+--------------+-----------+-----------+--------+\n|customerNumber|checkNumber|paymentDate|  amount|\n+--------------+-----------+-----------+--------+\n|           103|   HQ336336| 2004-10-19| 6066.78|\n|           103|   JM555205| 2003-06-05|14571.44|\n|           103|   OM314933| 2004-12-18| 1676.14|\n|           112|   BO864823| 2004-12-17|14191.12|\n|           112|    HQ55022| 2003-06-06|32641.98|\n+--------------+-----------+-----------+--------+\nonly showing top 5 rows\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.payments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 13107.20it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_payments.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.payments\nLoaded data from cleaned_productlines.csv:\n+------------+--------------------+---------------+-----+\n| productLine|     textDescription|htmlDescription|image|\n+------------+--------------------+---------------+-----+\n|Classic Cars|Attention car ent...|           NULL| None|\n| Motorcycles|Our motorcycles a...|           NULL| None|\n|      Planes|Unique, diecast a...|           NULL| None|\n|       Ships|The perfect holid...|           NULL| None|\n|      Trains|Model trains are ...|           NULL| None|\n+------------+--------------------+---------------+-----+\nonly showing top 5 rows\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.productlines\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 10205.12it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_productlines.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.productlines\nLoaded data from cleaned_products.csv:\n+-----------+--------------------+------------+------------+--------------------+--------------------+---------------+-------------+--------+------+\n|productCode|         productName| productLine|productScale|       productVendor|  productDescription|quantityInStock|warehouseCode|buyPrice|  MSRP|\n+-----------+--------------------+------------+------------+--------------------+--------------------+---------------+-------------+--------+------+\n|   S10_1678|1969 Harley David...| Motorcycles|        1:10|     Min Lin Diecast|This replica feat...|           7933|            a|   48.81| 95.70|\n|   S10_1949|1952 Alpine Renau...|Classic Cars|        1:10|Classic Metal Cre...|Turnable front wh...|           7305|            b|   98.58|214.30|\n|   S10_2016|1996 Moto Guzzi 1...| Motorcycles|        1:10|Highway 66 Mini C...|Official Moto Guz...|           6625|            a|   68.99|118.94|\n|   S10_4698|2003 Harley-David...| Motorcycles|        1:10|   Red Start Diecast|Model features, o...|           NULL|         NULL|    NULL|  NULL|\n|     ,,,,\\n|                NULL|        NULL|        NULL|                NULL|                NULL|           NULL|         NULL|    NULL|  NULL|\n+-----------+--------------------+------------+------------+--------------------+--------------------+---------------+-------------+--------+------+\nonly showing top 5 rows\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.products\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 9489.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_products.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.products\nLoaded data from cleaned_warehouses.csv:\n+-------------+-------------+---------------+\n|warehouseCode|warehouseName|warehousePctCap|\n+-------------+-------------+---------------+\n|            a|        North|             72|\n|            b|         East|             67|\n|            c|         West|             50|\n|            d|        South|             75|\n+-------------+-------------+---------------+\n\nUploading data to BigQuery table: shining-bazaar-436810-p2.model_cars.warehouses\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00, 11983.73it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from cleaned_warehouses.csv uploaded successfully to shining-bazaar-436810-p2.model_cars.warehouses\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set your GCP project and dataset\n",
    "project_id = \"shining-bazaar-436810-p2\"\n",
    "dataset_id = \"model_cars\"\n",
    "\n",
    "# Path to your service account key in DBFS (adjust to your file's location)\n",
    "gcp_credentials_path = \"xrbams_key.json\"\n",
    "\n",
    "# Authenticate using credentials\n",
    "credentials = service_account.Credentials.from_service_account_file(gcp_credentials_path)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BigQueryUploader\").getOrCreate()\n",
    "\n",
    "# Dictionary mapping CSV filenames to BigQuery table names\n",
    "csv_to_bq_table = {\n",
    "    \"cleaned_customers.csv\": \"customers\",\n",
    "    \"cleaned_employees.csv\": \"employees\",\n",
    "    \"cleaned_offices.csv\": \"offices\",\n",
    "    \"cleaned_orderdetails.csv\": \"orderdetails\",\n",
    "    \"cleaned_orders.csv\": \"orders\",\n",
    "    \"cleaned_payments.csv\": \"payments\",\n",
    "    \"cleaned_productlines.csv\": \"productlines\",\n",
    "    \"cleaned_products.csv\": \"products\",\n",
    "    \"cleaned_warehouses.csv\": \"warehouses\"\n",
    "}\n",
    "\n",
    "# Iterate over the files and corresponding BigQuery tables\n",
    "for csv_file, table_name in csv_to_bq_table.items():\n",
    "    file_path = f\"dbfs:/tmp/{csv_file}\"\n",
    "    \n",
    "    # Read each CSV file into a Spark DataFrame\n",
    "    df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "    \n",
    "    # Show the data (for debugging purposes)\n",
    "    print(f\"Loaded data from {csv_file}:\")\n",
    "    df.show(5)  # Show the first 5 rows\n",
    "    \n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    pdf = df.toPandas()\n",
    "    \n",
    "    # Define the full table name for BigQuery\n",
    "    full_table_name = f\"{project_id}.{dataset_id}.{table_name}\"\n",
    "    \n",
    "    # Upload data to BigQuery using pandas-gbq\n",
    "    print(f\"Uploading data to BigQuery table: {full_table_name}\")\n",
    "    pandas_gbq.to_gbq(\n",
    "        pdf, \n",
    "        full_table_name, \n",
    "        project_id=project_id, \n",
    "        credentials=credentials, \n",
    "        if_exists='replace'  # Use 'append' if you don't want to overwrite existing data\n",
    "    )\n",
    "\n",
    "    print(f\"Data from {csv_file} uploaded successfully to {full_table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "to_bigquery",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
